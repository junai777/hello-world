模拟面试地址https://bitable.feishu.cn/appwBhd98QJ9cpMggHc5QMVHlfd?from=navigation_button_1_trial&table=tbleuLHpPcQdvL4F&view=vew9iquA45
题目https://codetop.cc/#/home
系统设计https://www.bookstack.cn/read/system-design/README.md


进程线程概念
进程线程通信
IO模型
微服务
注册中心
服务降级原理
分布式事务实现原理       https://www.cnblogs.com/bluemiaomiao/p/11216380.html
redismysql消息队列一致性   https://zhuanlan.zhihu.com/p/91770135
java map resize       https://zhuanlan.zhihu.com/p/55890890
乐观锁与悲观锁实现原理   https://www.cnblogs.com/X-knight/p/10669934.html#_label2
lock与AQS
可重入锁原理
atomic类
怎么设计线程池     https://juejin.cn/post/6844903494223151112#heading-7
volatile synchronized实现原理
读写屏障
类加载机制
双亲委派
JMM内存模型
GC(CMS,G1,ZGC)
jdk8新特性stream及性能问题
ioc,aop,bean生命周期

分库分表  https://juejin.cn/post/6844903929457672205

mysql存储引擎及特性(myisam,innodb,memory,federate)
b-b+树
索引
锁与并发
ACID
隔离级别及原理
MVCC与间隙锁
RR解决幻读
提高性能(范式与反范式,sql优化,索引等)
覆盖索引
回表查询
最左匹配原则

redis数据结构
string存储原理
zset跳表原理及复杂度
哨兵机制
主从复制(全量增量)
持久化机制
aof的rewrite机制(怎么保持一致性)
渐进式哈希
缓存击穿雪崩穿透
分布式锁
setnx与expire合并成原子操作

MQ拓扑
不同MQ举例和差异
MQ集群逻辑

CICD的理解
Jenkins

怎么对推送的视频进行去重(user_id, video_id)
数据量很大了怎么办(布隆过滤器)
怎么保证扩容的并发性能且保证原子性
rpc与消息队列的区别
设计一个统计用户聊天人数与聊天数的系统(set)
设计一个抢红包系统(rpush&lpop)
http与redis通信方式(RESP)差异
为什么redis不用http
http数据包的格式
mysql与redis的高可用与数据分表
redis数据槽之间数据怎么迁移
类似朋友圈系统的设计
有大V网红怎么处理(热点数据池)


反射
注解
抽象工厂模式
迭代器模式
适配器模式
CAS
epoll
IO多路复用



xss
csrf

如何设计分布式缓存
如何设计高并发系统
如何设计消息队列
如何设计秒杀系统

进程间通信：无名管道、有名管道、信号、消息队列     https://www.jianshu.com/p/c1015f5ffa74      https://juejin.cn/post/6896383206350831630#heading-23
socket各个函数含义
es refresh  flush  translog https://www.jianshu.com/p/15837be98ffd
es query 原理  任务队列
协程

程序的运行

用户态和内核态 https://juejin.cn/post/6860166482710495246

线程间的通信方式: 互斥量,信号量,事件
协程的概念
分段和分页以及对应的场景
ping与TTL
工厂模式和观察者模式
中断的分类
软中断和硬中断
红黑树最重要的性质：从根到叶子的最长的可能路径小于等于最短的可能路径的两倍长。
创建对象的方式
多态 编译时多态 运行时多态
单例模式(线程安全)

写一个死锁的例子

volatile
synchronized
ReetrantLock实现方式




Java 线程池有哪些参数
线程池的工作过程
roaring bitmap
java基础https://juejin.cn/post/6932040503047225351#heading-22


mysql char vchar
rabbitmq如何保证至少一次  如果想要保证至少一次投递，使用队列镜像，持久的队列，持久的消息，发布者ACK，mandatory标志位，手动消费者ACK；
mq消息丢失
生产者丢消息：使用confirm模式，使用ack来确认是否成功投递
消息队列丢数据：使用镜像集群+持久化落盘，待写到主片和全部副片时才返回ack
消费者丢数据：消费者返回ack才处理成功，否则不断重发


可重复读和幻读
分布式id生成雪花算法的时间漂移  依赖与系统时间的一致性，如果系统时间被回调，或者改变，可能会造成id冲突或者重复。
redis数据淘汰策略实现
分库分表策略
http头进行数据标识
TCP粘包
CDN 内容分发网络，可缓存静态Web内容和流媒体内容，实现内容的边缘传播和存储，以便用户的就近访问。


三色标记
CMS：写屏障+增量更新，即有黑色指向白色时记录下这个引用
G1:写屏障+SATB，即灰色断开指向白色的时候，记录下这个引用

意向锁
为了避免申请表锁时去遍历整个表检测行锁。
即对某个行做修改会先获得意向共享锁
申请表锁时会先获得意向排他锁
如果有事务对某个行做修改，会有意向共享锁
这时申请表锁就被阻塞

next key lock
mvcc在RR RC下的不同
RR的读视图在事务开始时建立
RC的读视图在每条语句开始时建立

mysql各种日志
双写  
InnoDB 的Page Size一般是16KB，写文件是以4KB作为单位的，在极端情况下（比如断电）往往并不能保证这一操作的原子性16K的数据，
写入4K 时，发生了系统断电/os crash ，只有一部分写是成功的，这种情况下就是 partial page write 问题。
有人会想到系统恢复后MySQL可以根据redolog 进行恢复，而mysql在恢复的过程中是检查page的checksum，checksum就是pgae的最后事务号，发生partial page write 问题时，p
age已经损坏，找不到该page中的事务号，就无法恢复。
为了解决 partial page write 问题
1.当mysql将脏数据flush到data file的时候, 先使用memcopy 将脏数据复制到内存中的double write buffer ，
2.通过double write buffer再分2次，每次写入1MB到共享表空间，
3.然后马上调用fsync函数，同步到磁盘上，避免缓冲带来的问题。

缓存机制及所用数据结构
https://www.cnblogs.com/jiangxu67/p/3765708.html

redo log 为什么不直接落盘
组提交节省IO

fork的子进程拥有父进程的文件描述符吗？
子进程会继承父进程的文件描述符，父子进程对同一个文件进行写，将共享文件偏移，所以如果父子进程的其中一个使用了fclose关闭了文件描述符，实际上还有另外一个进程打开了test.txt文件。
如果父子进程都对文件进行写，并不会产生两个不同的文件，而是会对同一个文件进行写，因此运行后会在同一个文件里出现父子进程写的内容

fork
子进程拥有父进程的数据段，代码段，堆栈的副本
其实使用了写时复制技术，即fork之后，子进程名义上拥有父进程的副本，但是实际上和父进程共用，只有当父子进程中有一个试图修改这些区域时，才会以页为单位创建一个真正的副本。


mvcc
多版本并发控制,目的在于提高数据库高并发场景下的吞吐性能. 
不同的事务在并发过程中，SELECT 操作可以不加锁而是通过 MVCC 机制读取指定的版本历史记录，并通过一些手段保证保证读取的记录值符合事务所处的隔离级别，从而解决并发场景下的读写冲突。
快照读使用版本号加活跃数组构成的视图来实现，当前读对相应的行和间隙加锁。确保了读读不阻塞，读写不阻塞，只是写写阻塞。

线程池队列
(有界队列)ArrayBlockingQueue,
(无界队列)LinkedBlockingQeque,
(优先级队列)PriorityBlockingQueue，

线程池参数设置
IO密集型 2*CPU核数
计算密集型 CPU核数+1



AQS原理
AbstractQueuedSynchronizer 抽象的队列式同步器
AQS的核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并将共享资源设置为锁定状态，如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制AQS是用CLH队列锁实现的，即将暂时获取不到锁的线程加入到队列中。
CLH（Craig，Landin，and Hagersten）队列是一个虚拟的双向队列，虚拟的双向队列即不存在队列实例，仅存在节点之间的关联关系。
AQS是将每一条请求共享资源的线程封装成一个CLH锁队列的一个结点（Node），来实现锁的分配。
用大白话来说，AQS就是基于CLH队列，用volatile修饰共享变量state，线程通过CAS去改变状态符，成功则获取锁成功，失败则进入等待队列，等待被唤醒。
AQS 定义了两种资源共享方式：
1.Exclusive：独占，只有一个线程能执行，如ReentrantLock
2.Share：共享，多个线程可以同时执行，如Semaphore、CountDownLatch、ReadWriteLock，CyclicBarrier
state   使用了volatile修饰
https://tech.meituan.com/2019/12/05/aqs-theory-and-apply.html

段式
按照程序自身的逻辑关系划分为若干个段，内存分配规则：以段为单位进行分配，每个段在内存中占连续空间，但各段之间可以不相邻
分段对用户是可见的
优点：很方便按照逻辑模块实现信息的共享和保护
缺点：段长过大，很难为其分配连续的内存空间

页式
分页对用户不可见
优点：内存空间利用率高，不会产生外部碎片，只有少量内部碎片
缺点：不方便按照逻辑模块实现信息的共享和保护

段页式
既可以方便按照逻辑模块实现信息的共享和保护，又可以实现高的内存利用率


GET和POST区别
GET产生一个TCP数据包，浏览器会把http header和data一并发送出去，服务器响应200(返回数据);
POST产生两个TCP数据包，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok(返回数据)。
get把请求的数据放在url上
post把数据放在HTTP的包体内

HTTP
请求行：方法，版本，URI
首部字段：数据格式，数据长度
实体数据

响应行：版本，状态码
首部字段：数据格式，数据长度
实体数据


epoll的ET和LT区别
(LT) 水平触发：有数据可读 读事件一直触发
(ET) 边沿触发：状态变化时触发读事件
使用场景：nginx使用ET，其它场景使用容易遗漏数据。其它场景使用LT


惊群效应
多进程（多线程）在同时阻塞等待同一个事件的时候（休眠状态），如果等待的这个事件发生，那么他就会唤醒等待的所有进程（或者线程），
但是最终却只可能有一个进程（线程）获得这个时间的“控制权”，对该事件进行处理，而其他进程（线程）获取“控制权”失败，只能重新进入休眠状态，这种现象和性能浪费就叫做惊群。
惊群消耗了什么：
1系统对用户进程/线程频繁地做无效的调度，上下文切换系统性能大打折扣。
2为了确保只有一个线程得到资源，用户必须对资源操作进行加锁保护，进一步加大了系统开销。
1）accept（）惊群
当多个进程/线程都阻塞在对同一个socket的接受调用上时，当有一个新的连接到来，内核只会唤醒一个进程，其他进程保持休眠，压根就不会被唤醒。
2）epoll惊群：
epoll_wait的惊群确实存在，因为可能一个文件会由多个进程来读写
解决方法：
SO_REUSEPORT特性，该特性支持多个进程或者线程绑定到同一端口，提高服务器程序的性能，允许多个套接字bind()以及listen()同一个TCP或UDP端口，并且在内核层面实现负载均衡。
在使用SO_REUSEPORT后，多个进程可以同时监听同一个IP：端口，然后由内核决定将新链接发送给哪个进程，显然会降低每个工人接收新链接时锁竞争



如何避免竞争资源(racing condition)
使用锁，对共享区代码进行线程同步
synchronized
ReentrantLock
Semaphore
wait()/notify()事件
而要避免因使用了锁，导致线程阻塞，从而导致频繁的上下文切换
主要有以下措施：
1.降低锁的粒度：读锁写锁分离，分段锁
2.减少锁的持有时间：将无关代码移出共享区
3.使用事件通知时不要唤醒所有线程，而是唤醒特定线程
4.合理设置线程池大小


推拉模式

pull
优点：客户端可以依据自己的消费能力进行消费
缺点：数据有延时

push
优点：服务端主动推送给客户端，及时性很高
缺点：客户端消费能力低了得话会造成大量消息堆积

select缺点
1.每次调用select，都需要把fd集合从用户态拷贝到内核态，fd越多开销则越大；
2.每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大
3.select支持的文件描述符数量有限，默认是1024。参见/usr/include/linux/posix_types.h中的定义：


epoll比select高效的一个原因是
1.fd是共享在用户态和内核态之间的，所以可以不必进行从用户态到内核态的一个拷贝，节省了用户态和内核态间数据拷贝的资源消耗。原理是使用内存映射技术(mmap)
即进程直接通过页表将监听的fd映射到内存，即可直接访问内存中存储的fd，而且内核也可以访问到这段内存，从而节省了用户态和内核态的数据拷贝。
2.通过每个fd定义的回调函数来实现的，只有就绪的fd才会执行回调函数，而这个回调函数会把就绪的fd加入一个就绪链表。
那么当我们调用epoll_wait时，epoll_wait只需要检查链表中是否有存在就绪的fd即可。I/O的效率不会随着监视fd的数量的增长而下降
3.epoll因为不用轮询，所以没有监听数量限制



cookie
记录了用户浏览的上下文信息，比如浏览到哪个页面

禁用cookie后可以把sessionId写url中使用session

cookie跨域: 二级域名可访问
因为存在各种攻击，因此浏览器采用了同源策略，即cookie只能访问同源的url
可以采用CORS进行跨域访问
对于简单请求，浏览器直接发出CORS请求。具体来说，就是在头信息之中，增加一个Origin字段。
Origin字段用来说明，本次请求来自哪个源（协议 + 域名 + 端口）。服务器根据这个值，决定是否同意这次请求。
Access-Control-Allow-Origin 表示接受哪些域名的请求
Access-Control-Allow-Credentials  表示是否允许发送Cookie

CDN
内容分发网络，依靠部署在各地的边缘服务器，使用户就近获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率

网站响应慢排查
1.响应时间长
DNS解析慢
建立连接慢

2.执行时间慢
CPU内存磁盘数据库

3.加载时间慢
文件大，带宽小


键盘按下一个a之后的整体过程
1.电路板连通，接口通过驱动程序向中断器发出中断请求
2.CPU发现中断之后进入中断处理，到接口处取出键值
3.键值送到显卡，显卡送到屏幕显示



在一个很大的日志文件中查找到出现最多的ip并且记录次数
mapreduce
通过Hash码进行分片, IP 相同的肯定在一个文件中，之后统计每个文件中出现最多次数的 IP, 合并到一个文件中, 最后统计   合并的文件, 取最终结果


100亿数据找出最大的1000个数字
1.最小堆来实现
2.mapreduce
把数据分为若干个分区，然后对各个分区进行统计保留前1000个数，然后把各个分区的结果合并起来



epoll惊群问题

volatile总线风暴

MESI一致性协议

几种分布式锁的优缺点

raft zab一致性算法

实现RPC框架
实现消息中间件
实现redis



设计获取用户的好友24小时内TOP查询排行榜的功能
4个表user_id存储用户信息比如用户id、姓名、工号, user_query用户id、用户查询、时间, user_follow用户id、关注的用户id, user_latest_table用户id、关注的用户最频繁的查询
1000个用户, 1个用户关注100人
一个用户24小时约产生100条查询
即对于每个用户的获取好友24小时内频繁查询排行榜，
都会生成100*100=1万条统计数据，进行查询统计获取最频繁查询排行TOP10
拉起后台线程刷新一次用户的TOP查询，具体方法是
每个用户是订阅者，订阅了多个用户的信息，
可以通过订阅表查询出订阅的多个用户的信息，然后再获取多个用户24小时内的所有查询
在24小时内产生的查询大概一万条，经过使用map统计这些查询，把统计结果更新到这个用户的TOP查询中



object的方法
clone、equals、finalize、getClass、hashCode、notify、notifyAll、toString、wait

final、finally、finalize的区别
final用于声明属性，方法和类，分别表示属性不可变，方法不可覆盖，类不可继承。
finally是异常处理语句结构的一部分，表示总是执行。
finalize是Object类的一个方法，在垃圾收集器执行的时候会调用被回收对象的此方法，供垃圾收集时的其他资源回收，例如关闭文件等。


缺页中断
进程线性地址空间里的页面不必常驻内存，在执行一条指令时，如果发现他要访问的页没有在内存中（存在位为0），那么停止该指令的执行，并产生一个页不存在异常，
对应的故障处理程序可通过从外存加载加载该页到内存的方法来排除故障，之后，原先引起的异常的指令就可以继续执行，而不再产生异常。



线程安全



Runnable和Callable的区别是，
(1)Callable规定的方法是call(),Runnable规定的方法是run().
(2)Callable的任务执行后可返回值，而Runnable的任务是不能返回值得
(3)call方法可以抛出异常，run方法不可以



幂等性
1.唯一索引
2.乐观锁
3.防重表


CPU的实模式和保护模式
实模式：两个 16 位的值如何组合成一个 20 位的地址：段寄存器所提供的段基址先向左移 4 位，然后再与 16 位的段偏移量相加
保护模式：保护进程地址空间，有段表来指向段基地址的位置，而不是段基地址直接参与计算


变量的声明有两种情况：
1、一种是需要建立存储空间的。例如：int a 在声明的时候就已经建立了存储空间。
2、另一种是不需要建立存储空间的。 例如：extern int a 其中变量a是在别的文件中定义的。



设计缩短的url

数据库表：url表、user表，使用user_id联结
使用非关系型数据库

可用MD5或SHA256对URL进行计算编码
可通过url+user_id对url进行防重处理

基于hash(id)取模进行分库
使用redis缓存20%的热点数据，使用切片集群+哨兵集群等保证缓存层可用性


设计Youtube
视频上传接口
视频搜索接口
视频观看接口
好评差评评论接口

数据库
视频原数据表：视频id，标题，大小，评论等
用户数据表：用户id，用户名，邮箱等

视频存HDFS
把视频复制到多个从库上分担读请求
使用一主多从型集群分担读视频请求

某个用户若有很多订阅者，则放入缓存
缓存层使用一致性hash算法寻址

使用CDN预先缓存热点视频，实现加速

大于1秒之后属于慢查询

g1垃圾回收器
分代垃圾回收器
把堆分为若干个区域，区域分eden,survivor,old,humongous四种
新生代采用复制算法，老年代采用标记整理算法
回收的步骤为:
1.初始标记
2.并发标记
3.最终标记
4.清除垃圾

使用了rembered set结构来记录引用了此区域的老年代区域，来避免扫描整个老年代
通过Remember Set我们可以找到哪些对象引用了当前region的对象

标记算法使用了三色标记算法
即黑色为根对象,灰色为被扫描对象,白色为不可达对象
G1使用了写屏障+删除的时候记录所有的对象的方法来避免对象丢失
即开始标记时生成快照标记存活对象
然后并发标记时所改变的对象入队
最后此次改变的对象在下次被收集

G1分为young gc模式和mixed gc模式
前者只对新生代回收
后者对新老都进行回收





TCC
先是服务调用链路依次执行 Try 逻辑。
如果都正常的话，TCC 分布式事务框架推进执行 Confirm 逻辑，完成整个事务。
如果某个服务的 Try 逻辑有问题，TCC 分布式事务框架感知到之后就会推进执行各个服务的 Cancel 逻辑，撤销之前执行的各种操作。


mysql的线程有哪几个
master thread: 核心后台线程，异步刷新到硬盘。脏页刷新，合并插入缓冲，刷新日志到磁盘
IO thread: 处理IO请求的，读主库bin log 到从库
purge thread: 回收undo页
page cleaner thread: 刷脏页


mysql内存
读写都在缓冲区buffer pool中，定时会把脏页刷到磁盘
buffer pool: 数据页，索引页，插入缓冲，锁信息，自适应哈希索引，数据字典信息
使用LRU算法管理
redo log buffer：事务提交时刷磁盘，每秒刷磁盘，满一半时刷磁盘
额外内存池

check point刷盘
更新数据是写缓冲池中数据页，然后写redo log buffer
然后使用check point技术刷盘，即缓冲池不够用时或redo log不可用时
通过LSN标记版本



用户态通过系统调用切换到内核态：
CPU 寄存器里原来用户态的指令位置，需要先保存起来。
接着，为了执行内核态代码，CPU 寄存器需要更新为内核态指令的新位置。最后才是跳转到内核态运行内核任务。
而系统调用结束后，CPU 寄存器需要恢复原来保存的用户态，然后再切换到用户空间，继续运行进程。
所以，一次系统调用的过程，其实是发生了两次 CPU 上下文切换。

进程上下文切换，是指从一个进程切换到另一个进程运行。而系统调用过程中一直是同一个进程在运行。


CPU上下文：CPU寄存器和程序计数器(PC)，作用是记录任务从哪里加载、又从哪里开始运行
CPU上下文切换:前一个任务的CPU上下文保存起来，加载下一个任务的CPU上下文到CPU寄存器和PC，然后跳转到PC指向的地址，执行指令

进程上下文：不仅包括了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的状态

进程上下文切换：在保存当前进程的内核状态和 CPU 寄存器之前，需要先把该进程的虚拟内存、栈等保存下来；
而加载了下一进程的内核态后，还需要刷新进程的虚拟内存和用户栈。
因为页表


线程上下文切换
中断上下文切换



API限速(每秒限制请求1000)
使用queue记录请求的时间戳，当queue的头元素和当前时间相差大于一秒时，就poll头元素，循环执行
然后如果此时队列长度小于1000，则允许请求，并把请求的时间加入队列
如何大于1000，则拒绝请求


设计一个web服务器
使用多reactor多进程模式
1.父进程mainReactor对象通过select监控连接建立事件，收到事件后通过acceptor接收，将新的连接分配给某个子进程
2.子进程subReactor将mainReactor分配的连接加入连接队列进行监听，并创建一个Handler用于处理连接的各种事件
3.当有新的事件发生时，subReactor会调用连接对应的Handler来进行响应
4.Handler完成业务处理


对于SYN泛洪攻击的防范，优化主机系统设置是常用的手段。如降低SYN timeout时间，使得主机尽快释放半连接的占用；
又比如采用SYN cookie设置，如果短时间内连续收到某个IP的重复SYN请求，则认为受到了该IP的攻击，丢弃来自该IP的后续请求报文。











